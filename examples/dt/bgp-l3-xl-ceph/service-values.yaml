kind: ConfigMap
apiVersion: v1
metadata:
    name: service-values
    annotations:
        config.kubernetes.io/local-config: 'true'
data:
  topologyRef:
    name: default-spread-pods
  preserveJobs: false
  neutron:
    customServiceConfig: '[DEFAULT]

        vlan_transparent = true

        debug = true

        [ovs]

        igmp_snooping_enable = true

        '
  octavia:
    amphoraImageContainerImage: quay.io/gthiemonge/octavia-amphora-image
    apacheContainerImage: registry.redhat.io/ubi9/httpd-24:latest
    enabled: false
    octaviaAPI:
      customServiceConfig: '[controller_worker]

          loadbalancer_topology=ACTIVE_STANDBY

          '
      networkAttachments:
      - internalapi
    octaviaHealthManager:
      customServiceConfig: '[controller_worker]

          loadbalancer_topology=ACTIVE_STANDBY

          '
      networkAttachments:
      - octavia
    octaviaHousekeeping:
      customServiceConfig: '[controller_worker]

          loadbalancer_topology=ACTIVE_STANDBY

          '
      networkAttachments:
      - octavia
    octaviaWorker:
      customServiceConfig: '[controller_worker]

          loadbalancer_topology=ACTIVE_STANDBY

          '
      networkAttachments:
      - octavia
  ovn:
    ovnController:
      external-ids:
        enable-chassis-as-gateway: false

  cinder:
    customServiceConfig: |
      [DEFAULT]
      storage_availability_zone = az0
  cinderAPI:
    replicas: 3
  cinderBackup:
    replicas: 1
    topologyRef:
      name: azone-node-affinity
    customServiceConfig: |
      [DEFAULT]
      backup_driver = cinder.backup.drivers.ceph.CephBackupDriver
      backup_ceph_conf = /etc/ceph/az0.conf
      backup_ceph_pool = backups
      backup_ceph_user = openstack
  cinderVolumes:
    az0:
      topologyRef:
        name: azone-node-affinity
      customServiceConfig: |
        [DEFAULT]
        enabled_backends = ceph
        glance_api_servers = https://glance-az0-internal.openstack.svc:9292
        [ceph]
        volume_backend_name = ceph
        volume_driver = cinder.volume.drivers.rbd.RBDDriver
        rbd_ceph_conf = /etc/ceph/az0.conf
        rbd_user = openstack
        rbd_pool = volumes
        rbd_flatten_volume_from_snapshot = False
        rbd_secret_uuid = CHANGE_TO_CEPH_FSID_AZ0
        rbd_cluster_name = az0
        backend_availability_zone = az0
    az1:
      topologyRef:
        name: bzone-node-affinity
      customServiceConfig: |
        [DEFAULT]
        enabled_backends = ceph
        glance_api_servers = https://glance-az1-internal.openstack.svc:9292
        [ceph]
        volume_backend_name = ceph
        volume_driver = cinder.volume.drivers.rbd.RBDDriver
        rbd_ceph_conf = /etc/ceph/az1.conf
        rbd_user = openstack
        rbd_pool = volumes
        rbd_flatten_volume_from_snapshot = False
        rbd_secret_uuid = CHANGE_TO_CEPH_FSID_AZ1
        rbd_cluster_name = az1
        backend_availability_zone = az1
    az2:
      topologyRef:
        name: czone-node-affinity
      customServiceConfig: |
        [DEFAULT]
        enabled_backends = ceph
        glance_api_servers = https://glance-az2-internal.openstack.svc:9292
        [ceph]
        volume_backend_name = ceph
        volume_driver = cinder.volume.drivers.rbd.RBDDriver
        rbd_ceph_conf = /etc/ceph/az2.conf
        rbd_user = openstack
        rbd_pool = volumes
        rbd_flatten_volume_from_snapshot = False
        rbd_secret_uuid = CHANGE_TO_CEPH_FSID_AZ2
        rbd_cluster_name = az2
        backend_availability_zone = az2

  preserveJobs: false
  swift:
    enabled: false

  memcached:
    templates:
      memcached:
        replicas: 3
      memcached-azone:
        replicas: 3
        topologyRef:
          name: azone-node-affinity
      memcached-bzone:
        replicas: 3
        topologyRef:
          name: bzone-node-affinity
      memcached-czone:
        replicas: 3
        topologyRef:
          name: czone-node-affinity

  glance:
    # This glance.customServiceConfig is only here
    # to pacify ../../../../dt/bgp/ and is ignored
    # since a separate customServiceConfig is in
    # place for each glance
    customServiceConfig: ""
    # This glance.default.replicas is only here
    # to pacify ../../../../dt/bgp/ and is ignored;
    # replicas are set separately for each glance
    default:
      replicas: 3
    # Ensure keystoneEndpoint is set
    keystoneEndpoint: default
    # There are 4 glanceAPIs keys below (default, az{0,1,2})
    glanceAPIs:
      default:
        replicas: 3
        customServiceConfig: |
          [DEFAULT]
          enabled_import_methods = [web-download,copy-image,glance-direct]
          enabled_backends = az0:rbd,az1:rbd,az2:rbd
          [glance_store]
          default_backend = az0
          [az0]
          rbd_store_ceph_conf = /etc/ceph/az0.conf
          store_description = "az0 RBD backend"
          rbd_store_pool = images
          rbd_store_user = openstack
          rbd_thin_provisioning = True
          [az1]
          rbd_store_ceph_conf = /etc/ceph/az1.conf
          store_description = "az1 RBD backend"
          rbd_store_pool = images
          rbd_store_user = openstack
          rbd_thin_provisioning = True
          [az2]
          rbd_store_ceph_conf = /etc/ceph/az2.conf
          store_description = "az2 RBD backend"
          rbd_store_pool = images
          rbd_store_user = openstack
          rbd_thin_provisioning = True
        networkAttachments:
          - storage
        override:
          service:
            internal:
              metadata:
                annotations:
                  metallb.universe.tf/address-pool: internalapi
                  metallb.universe.tf/allow-shared-ip: internalapi
                  metallb.universe.tf/loadBalancerIPs: 172.17.0.80
              spec:
                type: LoadBalancer
        type: split
      az0:
        topologyRef:
          name: azone-node-affinity
        customServiceConfig: |
          [DEFAULT]
          enabled_import_methods = [web-download,copy-image,glance-direct]
          enabled_backends = az0:rbd
          [glance_store]
          default_backend = az0
          [az0]
          rbd_store_ceph_conf = /etc/ceph/az0.conf
          store_description = "az0 RBD backend"
          rbd_store_pool = images
          rbd_store_user = openstack
          rbd_thin_provisioning = True
        networkAttachments:
          - storage
        override:
          service:
            internal:
              metadata:
                annotations:
                  metallb.universe.tf/address-pool: internalapi
                  metallb.universe.tf/allow-shared-ip: internalapi
                  metallb.universe.tf/loadBalancerIPs: 172.17.0.81
              spec:
                type: LoadBalancer
        replicas: 2
        type: edge
      az1:
        topologyRef:
          name: bzone-node-affinity
        customServiceConfig: |
          [DEFAULT]
          enabled_import_methods = [web-download,copy-image,glance-direct]
          enabled_backends = az0:rbd,az1:rbd
          [glance_store]
          default_backend = az1
          [az1]
          rbd_store_ceph_conf = /etc/ceph/az1.conf
          store_description = "az1 RBD backend"
          rbd_store_pool = images
          rbd_store_user = openstack
          rbd_thin_provisioning = True
          [az0]
          rbd_store_ceph_conf = /etc/ceph/az0.conf
          store_description = "az0 RBD backend"
          rbd_store_pool = images
          rbd_store_user = openstack
          rbd_thin_provisioning = True
        networkAttachments:
          - storage
        override:
          service:
            internal:
              metadata:
                annotations:
                  metallb.universe.tf/address-pool: internalapi
                  metallb.universe.tf/allow-shared-ip: internalapi
                  metallb.universe.tf/loadBalancerIPs: 172.17.0.82
              spec:
                type: LoadBalancer
        replicas: 2
        type: edge
      az2:
        topologyRef:
          name: czone-node-affinity
        customServiceConfig: |
          [DEFAULT]
          enabled_import_methods = [web-download,copy-image,glance-direct]
          enabled_backends = az0:rbd,az2:rbd
          [glance_store]
          default_backend = az2
          [az2]
          rbd_store_ceph_conf = /etc/ceph/az2.conf
          store_description = "az2 RBD backend"
          rbd_store_pool = images
          rbd_store_user = openstack
          rbd_thin_provisioning = True
          [az0]
          rbd_store_ceph_conf = /etc/ceph/az0.conf
          store_description = "az0 RBD backend"
          rbd_store_pool = images
          rbd_store_user = openstack
          rbd_thin_provisioning = True
        networkAttachments:
          - storage
        override:
          service:
            internal:
              metadata:
                annotations:
                  metallb.universe.tf/address-pool: internalapi
                  metallb.universe.tf/allow-shared-ip: internalapi
                  metallb.universe.tf/loadBalancerIPs: 172.17.0.83
              spec:
                type: LoadBalancer
        replicas: 2
        type: edge
  manila:
    customServiceConfig: |
      [DEFAULT]
      storage_availability_zone = az0
    enabled: true
    manilaAPI:
      replicas: 3
      customServiceConfig: |
        [DEFAULT]
        enabled_share_protocols=nfs,cephfs
    manilaScheduler:
      replicas: 3
    manilaShares:
      az0:
        replicas: 1
        topologyRef:
          name: azone-node-affinity
        customServiceConfig: |
          [DEFAULT]
          enabled_share_backends = cephfs_az0
          enabled_share_protocols = cephfs
          [cephfs_az0]
          driver_handles_share_servers = False
          share_backend_name = cephfs_az0
          share_driver = manila.share.drivers.cephfs.driver.CephFSDriver
          cephfs_conf_path = /etc/ceph/az0.conf
          cephfs_cluster_name = az0
          cephfs_auth_id=openstack
          cephfs_volume_mode = 0755
          cephfs_protocol_helper_type = CEPHFS
          backend_availability_zone = az0
        networkAttachments:
          - storage
      az1:
        replicas: 1
        topologyRef:
          name: bzone-node-affinity
        customServiceConfig: |
          [DEFAULT]
          enabled_share_backends = cephfs_az1
          enabled_share_protocols = cephfs
          [cephfs_az1]
          driver_handles_share_servers = False
          share_backend_name = cephfs_az1
          share_driver = manila.share.drivers.cephfs.driver.CephFSDriver
          cephfs_conf_path = /etc/ceph/az1.conf
          cephfs_cluster_name = az1
          cephfs_auth_id=openstack
          cephfs_volume_mode = 0755
          cephfs_protocol_helper_type = CEPHFS
          backend_availability_zone = az1
        networkAttachments:
          - storage
      az2:
        replicas: 1
        topologyRef:
          name: czone-node-affinity
        customServiceConfig: |
          [DEFAULT]
          enabled_share_backends = cephfs_az2
          enabled_share_protocols = cephfs
          [cephfs_az2]
          driver_handles_share_servers = False
          share_backend_name = cephfs_az2
          share_driver = manila.share.drivers.cephfs.driver.CephFSDriver
          cephfs_conf_path = /etc/ceph/az2.conf
          cephfs_cluster_name = az2
          cephfs_auth_id=openstack
          cephfs_volume_mode = 0755
          cephfs_protocol_helper_type = CEPHFS
          backend_availability_zone = az2
        networkAttachments:
          - storage
  nova:
    customServiceConfig: |
      [DEFAULT]
      default_schedule_zone=az0
      [cinder]
      # for testing migration, would normally be false
      cross_az_attach=True
    metadataServiceTemplate:
      enabled: true
      # if cells per zone, set false
      # enabled: false
    cellTemplates:
      cell0:
        cellDatabaseAccount: nova-cell0
        hasAPIAccess: true
      # if cells per zone, uncomment topologyRef
      cell1:
        # topologyRef:
        #   name: azone-node-affinity
        cellDatabaseInstance: openstack-cell1
        cellDatabaseAccount: nova-cell1
        cellMessageBusInstance: rabbitmq-cell1
        conductorServiceTemplate:
          replicas: 1
        hasAPIAccess: true
        # if cells per zone, uncomment to deploy metadataService per zone
        # metadataServiceTemplate:
        #   enabled: true
        #   replicas: 3
      # if cells per zone, uncomment cell2 and cell3
      # cell2:
      #   topologyRef:
      #     name: bzone-node-affinity
      #   cellDatabaseInstance: openstack-cell2
      #   cellDatabaseAccount: nova-cell2
      #   cellMessageBusInstance: rabbitmq-cell2
      #   conductorServiceTemplate:
      #     replicas: 1
      #   hasAPIAccess: true
      #   metadataServiceTemplate:
      #     enabled: true
      #     replicas: 3
      # cell3:
      #   topologyRef:
      #     name: czone-node-affinity
      #   cellDatabaseInstance: openstack-cell3
      #   cellDatabaseAccount: nova-cell3
      #   cellMessageBusInstance: rabbitmq-cell3
      #   conductorServiceTemplate:
      #     replicas: 1
      #   hasAPIAccess: true
      #   metadataServiceTemplate:
      #     enabled: true
      #     replicas: 3
  rabbitmq:
    templates:
      rabbitmq:
        override:
          service:
            metadata:
              annotations:
                metallb.universe.tf/address-pool: internalapi
                metallb.universe.tf/loadBalancerIPs: 172.17.0.85
            spec:
              type: LoadBalancer
        replicas: 3
      rabbitmq-cell1:
        override:
          service:
            metadata:
              annotations:
                metallb.universe.tf/address-pool: internalapi
                metallb.universe.tf/loadBalancerIPs: 172.17.0.86
            spec:
              type: LoadBalancer
        replicas: 3
        # if cells per zone, uncomment topologyRef
        # topologyRef:
        #   name: azone-node-affinity
      # if cells per zone, uncomment cell2 and cell2
      # rabbitmq-cell2:
      #   override:
      #     service:
      #       metadata:
      #         annotations:
      #           metallb.universe.tf/address-pool: internalapi
      #           metallb.universe.tf/loadBalancerIPs: 172.17.0.87
      #       spec:
      #         type: LoadBalancer
      #   replicas: 3
      #   topologyRef:
      #     name: bzone-node-affinity
      # rabbitmq-cell3:
      #   override:
      #     service:
      #       metadata:
      #         annotations:
      #           metallb.universe.tf/address-pool: internalapi
      #           metallb.universe.tf/loadBalancerIPs: 172.17.0.88
      #       spec:
      #         type: LoadBalancer
      #   replicas: 3
      #   topologyRef:
      #     name: czone-node-affinity

  galera:
    templates:
      openstack:
        replicas: 3
        secret: osp-secret
        storageRequest: 5G
      openstack-cell1:
        replicas: 3
        secret: osp-secret
        storageRequest: 5G
      # if cells per zone, uncomment cell1 and cell3
      # openstack-cell2:
      #   replicas: 3
      #   secret: osp-secret
      #   storageRequest: 5G
      # openstack-cell3:
      #   replicas: 3
      #   secret: osp-secret
      #   storageRequest: 5G

  extraMounts:
    - extraVol:
        - propagation:
            - CinderVolume
            - CinderBackup
            - GlanceAPI
            - ManilaShare
          extraVolType: Ceph
          volumes:
            - name: ceph
              secret:
                secretName: ceph-conf-files
          mounts:
            - name: ceph
              mountPath: /etc/ceph
              readOnly: true
