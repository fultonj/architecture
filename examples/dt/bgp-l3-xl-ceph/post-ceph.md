# Update the control plane to use Ceph and complete the data plane deployment

## Assumptions

- The pre-Ceph [dataplane](data-plane.md) was already deployed and
  Ceph was manually installed on each nodeset in each zone afterwords

## Initialize

Switch to the "openstack" namespace
```
oc project openstack
```

## Define Ceph Secret

From each Ceph cluster extract a configuration file and keyring for
OpenStack clients to use. The example below shows how to do this for
the Ceph cluster in availability zone 0.
```
cephadm shell -- ceph config generate-minimal-conf > az0.conf
cephadm shell -- ceph auth get client.openstack > az0.client.openstack.keyring
```
Run commands like the above on the Ceph clusters in
availability zones 1 and 2.

An [empty Ceph secret](control-plane/empty-ceph-secret.yaml) should
already have been created and referenced by the
OpenStackDataplaneNodeSet and OpenStackControlPlane before Ceph was
deployed. Update it to use the new keryings generated by the previous
commands.
```
oc delete secret ceph-conf-files
oc create secret generic ceph-conf-files \
--from-file=az0.client.openstack.keyring \
--from-file=az0.conf \
--from-file=az1.client.openstack.keyring \
--from-file=az1.conf \
--from-file=az2.client.openstack.keyring \
--from-file=az2.conf
```

## Update Control Plane

Update the control plane to use the three Ceph clusters.
```
cd architecture/examples/dt/bgp-l3-xl-ceph
```
Edit the [service-values.yaml](service-values.yaml) file to suit your
environment.
```
vi service-values.yaml
```
The edit should include changing the following services to use the
three Ceph clusters.

- Cinder
- Glance
- Manila

Also, add `extraMounts` so that the above services can access
the Ceph configuration. The following command will provide insight
into how this file differs from the deployed version of the
control-plane.
```
diff service-values.yaml control-plane/service-values.yaml
```
The example [service-values.yaml](service-values.yaml) has the
following placeholders which should be changed to the actual Ceph FSID
from each cluster.

- `CHANGE_TO_CEPH_FSID_FROM_AZ0`
- `CHANGE_TO_CEPH_FSID_FROM_AZ1`
- `CHANGE_TO_CEPH_FSID_FROM_AZ2`

It's possible to get the actual values from the secret and set them to
shell variables.
```
AZ0=$(oc get secret ceph-conf-files -o json | jq -r '.data."az0.conf"' | base64 -d | grep fsid | sed -e 's/fsid = //')
AZ1=$(oc get secret ceph-conf-files -o json | jq -r '.data."az1.conf"' | base64 -d | grep fsid | sed -e 's/fsid = //')
AZ2=$(oc get secret ceph-conf-files -o json | jq -r '.data."az2.conf"' | base64 -d | grep fsid | sed -e 's/fsid = //')
```
and then use those variables to replace them in `service-values.yaml`
with `sed`.
```
sed -i \
    -e "s/CHANGE_TO_CEPH_FSID_AZ0/${AZ0}/g" \
    -e "s/CHANGE_TO_CEPH_FSID_AZ1/${AZ1}/g" \
    -e "s/CHANGE_TO_CEPH_FSID_AZ2/${AZ2}/g" \
    service-values.yaml
```
After updating service-values.yaml to have all of the values
appropriate for your environmnet, generate and apply the post-Ceph
control plane CR.
```
kustomize build > control-plane-post-ceph.yaml
oc apply -f control-plane-post-ceph.yaml
```
Wait for control plane to be available after updating
```
oc wait osctlplane controlplane --for condition=Ready --timeout=600s
```

## Define a custom OpenStackDataPlaneService

A custom
[OpenStackDataPlaneService](https://openstack-k8s-operators.github.io/openstack-operator/dataplane/#proc_creating-a-custom-service_dataplane)
per zone is necessary to configure the compute nodes in
each zone to use the correct Ceph cluster.

Change to the bgp-l3-xl-ceph post-ceph nova-ceph directory
```
cd architecture/examples/dt/bgp-l3-xl-ceph/edpm-post-ceph/nova-ceph/
```
Edit the values for each zone to change the Ceph FSID (similar to how
it was changed in the previous section).
```
vim r0/values.yaml
vim r1/values.yaml
vim r2/values.yaml
```
Generate the CRs for the `ConfigMap` and `OpenStackDataPlaneService`
for each zone.
```
kustomize build r0 > nova-custom-ceph-az0.yaml
kustomize build r1 > nova-custom-ceph-az1.yaml
kustomize build r2 > nova-custom-ceph-az2.yaml
```
Apply them.
```
oc apply -f nova-custom-ceph-az0.yaml
oc apply -f nova-custom-ceph-az1.yaml
oc apply -f nova-custom-ceph-az2.yaml
```

## Update Data Plane

Complete the deployment of the Compute and Networker nodes now that
Ceph has been deployed.

Change to the bgp-l3-xl-ceph post-ceph deployment directory
```
cd architecture/examples/dt/bgp-l3-xl-ceph/edpm-post-ceph/deployment/
```
Generate a dataplane deployment CRs
```
kustomize build computes/r0 > edpm-post-ceph-compute-deployment-r0.yaml
kustomize build computes/r1 > edpm-post-ceph-compute-deployment-r1.yaml
kustomize build computes/r2 > edpm-post-ceph-compute-deployment-r2.yaml
kustomize build networkers > edpm-post-ceph-networkers-deployment.yaml
```
Because the compute node each use a different
`OpenStackDataPlaneService` depending on their AZ, three different
deployments are necessary as each uses a different `servicesOverride`
list.

## Apply the deployment

Start the deployments.
```
oc apply -f edpm-post-ceph-compute-deployment-r0.yaml
oc apply -f edpm-post-ceph-compute-deployment-r1.yaml
oc apply -f edpm-post-ceph-compute-deployment-r2.yaml
oc apply -f edpm-post-ceph-networkers-deployment.yaml
```
Wait for data plane deployments to finish
```
oc wait osdpd edpm-post-ceph-computes-deployment-r0 --for condition=Ready --timeout=2400s
oc wait osdpd edpm-post-ceph-computes-deployment-r1 --for condition=Ready --timeout=2400s
oc wait osdpd edpm-post-ceph-computes-deployment-r2 --for condition=Ready --timeout=2400s
oc wait osdpd edpm-post-ceph-networkers-deployment --for condition=Ready --timeout=2400s
```

## Finalize Nova computes

Ask Nova to discover all compute hosts
```bash
oc rsh nova-cell0-conductor-0 nova-manage cell_v2 discover_hosts --verbose
```
